\documentclass{beamer}


\usepackage{alltt}%
\usetheme{Boadilla}
\usecolortheme{seahorse}

%\usepackage{listings}
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\usepackage[utf8]{inputenc}
\usepackage{default}

\usepackage{xcolor}%for color mixing

\usepackage{amsmath}%
\usepackage{amsfonts}%
\usepackage{amssymb}%
\usepackage{graphicx}

\usepackage{tikz}
\usepackage{multirow}
\usepackage{booktabs}

\setbeamertemplate{itemize/enumerate body begin}{\small}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[Logistic regression]{Generalized linear models and the logistic regression}
\author{Timoth\'ee Bonnet}
\institute{Thanks to BDSI for support!}
\date{\today}

\begin{document}

%\lstset{language=R}%code

\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{}
    \tableofcontents[currentsection,sectionstyle=show/show,subsectionstyle=show/shaded/hide]% down vote\tableofcontents[currentsection,currentsubsection,hideothersubsections,sectionstyle=show/hide,subsectionstyle=show/shaded/hide] 
  \end{frame}
}

<<Plot Options, echo=FALSE, message=FALSE>>=
#load(file = ".RData")
opts_knit$set(width=60)
opts_chunk$set(comment=NA, fig.width=8, fig.height=6, out.width='0.8\\textwidth',
               out.height='0.6\\textwidth',background='#D7DDEB', size="small")


szgr <- 2
szax <- 1.3
marr <- c(4, 4, 1, 1) + 0.1
setPar<-function(){
par(las=1,mar=marr, cex=szgr, cex.lab=szax , cex.axis=szax, lwd=2 ,pch=1, las=1)
}
setPar()
@

\begin{frame}{logiwhat?}

From Wikipedia:
\begin{quote}
The function was named in 1844 by Pierre François Verhulst, who studied it in relation to population growth. The initial stage of growth is approximately exponential (geometric); then, as saturation begins, the growth slows to linear (arithmetic), and at maturity, growth stops. Verhulst did not explain the choice of the term ``logistic", but it is presumably in contrast to the logarithmic curve and by analogy with arithmetic and geometric. His growth model is preceded by a discussion of arithmetic growth and geometric growth, and thus "logistic growth" is presumably named by analogy, logistic being from Ancient Greek logistikós, a traditional division of Greek mathematics. The term is unrelated to the military and management term logistics.
\end{quote}

The ``logit" function/unit/transform comes from the contraction of \textbf{log}istic un\textbf{it}.

\end{frame}
%%%%%%%%%%%%%%%


\begin{frame}{}
\maketitle

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{A simple linear model}
\centering

  \textbf{{\color{purple}{Response}} = {\color{blue}{Intercept}} + {\color{red}{Slope}} $\times$ {\color{orange}{Predictor}} + {\color{gray}{Error}}} \\

  <<lmprinc, echo=FALSE, dev='tikz'>>=
    setPar()
    set.seed(123)
    x <- rnorm(20)
    y <- 1 + x + rnorm(20)
    plot(x, y, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}")
    lm0 <- lm(y~x)
    abline(lm0, col="red", lwd=5)
    abline(h=coef(lm0)[1], lty=2, col="blue", lwd=5)
    newdat <- data.frame(x=seq(-2, 2, length.out = 100))
    newdat <- cbind(newdat,predict(lm0, se.fit = TRUE, newdata = newdat))
    newdat$CIu <- newdat$fit+newdat$se.fit*2
    newdat$CIl <- newdat$fit-newdat$se.fit*2
    polygon(x=c(newdat$x,rev(newdat$x)), y=c(newdat$CIl,rev(newdat$CIu)), col = rgb(0.8,0,0,0.2), border =NA)
    abline(v=0)
    abline(h=0)

    arrows(x0 = x, y0=y, y1=lm0$fitted.values, code=0, col="gray", lwd=3)
  @
\end{frame}
%%%%%%%%%%%

\begin{frame}[fragile]{A simple linear model failure: binary data}
\centering

  <<binlmprinc, echo=FALSE, dev='tikz'>>=
    setPar()
    set.seed(123)
    x <- rnorm(30)
    latent <- 1 + 2*x + rnorm(30, sd = 0.5)
    y <- 1/(1+exp(-latent))
    obs <- sapply(y, FUN=function(x){rbinom(1,1,x)})
    plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5))
    lm0 <- lm(y~x)
    newdat <- data.frame(x=seq(-3, 3, length.out = 100))
    newdat <- cbind(newdat,predict(lm0, se.fit = TRUE, newdata = newdat))
    newdat$CIu <- newdat$fit+newdat$se.fit*2
    newdat$CIl <- newdat$fit-newdat$se.fit*2
    polygon(x=c(newdat$x,rev(newdat$x)), y=c(newdat$CIl,rev(newdat$CIu)), col = rgb(0.8,0,0,0.2), border =NA)
    abline(lm0, col="red", lwd=5)
    abline(h=coef(lm0)[1], lty=2, col="blue", lwd=5)
    abline(v=0)
    abline(h=0)

    arrows(x0 = x, y0=obs, y1=lm0$fitted.values, code=0, col="gray", lwd=3)
  @
  
\end{frame}
%%%%%%%%%%%

\begin{frame}{What we want our model to do}
\centering

<<echo=FALSE>>=
set.seed(123)
x <- rnorm(30)
y <- 1 + 2*x + rnorm(30, 0, 0.2)
obs <- sapply(y, function(x){rbinom(1, 1, prob = 1/(1+exp(-x)))})
lrm0 <- glm(obs ~ 1 + x, family = "binomial")
newdat <- data.frame(x=seq(-3, 3, length.out = 100))
prlrm0 <- cbind(newdat, predict(lrm0,newdata =newdat, type = "link", se.fit = TRUE) )
@

<<logreg1, dev='tikz', echo=FALSE, fig.keep='last', fig.width=8, fig.height=6, out.width="0.8\\textwidth">>=
setPar()
plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5))
abline(h=c(0,1))
abline(v=0)
ybt <- 1/(1+exp(-prlrm0$fit))
lines(x = prlrm0$x, ybt, lwd=5, col='red')

ybtCIu <- 1/(1+exp(-(prlrm0$fit+2*prlrm0$se.fit)))
ybtCIl <- 1/(1+exp(-(prlrm0$fit-2*prlrm0$se.fit)))

polygon(x=c(prlrm0$x,rev(prlrm0$x)), y=c(ybtCIl,rev(ybtCIu)), col = rgb(0.8,0,0,0.2), border =NA)

pres <- predict(lrm0, type = "response")
segments(x0=x, y1 = pres, y0=obs, col="gray")

@

\end{frame}
%%%%%%%%%%%%

\begin{frame}{What we want our model to do}
\centering
\includegraphics[width=0.5\textwidth]{figure/logreg1-1.pdf}

\begin{block}{What we need:}
  \begin{enumerate}[<+->]
    \item Convert the predictor open scale ($-\infty$ to $+\infty$) to a bounded scale (0 to 1)
    \item Acknowledge discrete data
    \item Response variability depends on expected value
  \end{enumerate}
\end{block}
\end{frame}
%%%%%%%%%%%

\begin{frame}{That is what a Generalized Linear Model does}

\begin{block}{Vocabulary warning}
  \begin{itemize}
    \item General Linear Model (=linear model with several responses, multivariate)
    \item \textbf{Generalized Linear Model (=non-normal errors, and uncertainty dependent on the mean)} 
  \end{itemize}
\end{block}

\pause

\begin{block}{What a GLM is:}
  \begin{enumerate}[<+->]
    \item \textbf{Linear function} (reponse = intercept + slope $\times$ predictor \dots)
    \item ``\textbf{Link function}" = a map between the linear function ($-\infty$ to $+\infty$) and a probability distribution (from 0 to 1 for Bernouilli)
    \item \textbf{Probability distribution} (Bernouilli, Binomial, Poisson\dots) thought to generate the data (either 0 or 1 for Bernouilli)
  \end{enumerate}
\pause[\thebeamerpauses]

GLMs fit continuous expected response; we observe discrete realizations
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{What to do with logistic regression}
\centering
\only<1-2>{\includegraphics[width=0.5\textwidth]{figure/logreg1-1.pdf}}

\only<3>{
<<logreg2, dev='tikz', echo=FALSE, fig.keep='last', fig.width=8, fig.height=6, out.width="0.5\\textwidth",out.height="0.5\\textheight">>=
setPar()
plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5))
abline(h=c(0,1))
abline(v=0)
ybt <- 1/(1+exp(-prlrm0$fit))
lines(x = prlrm0$x, ybt, lwd=5, col='red')

ybtCIu <- 1/(1+exp(-(prlrm0$fit+2*prlrm0$se.fit)))
ybtCIl <- 1/(1+exp(-(prlrm0$fit-2*prlrm0$se.fit)))

polygon(x=c(prlrm0$x,rev(prlrm0$x)), y=c(ybtCIl,rev(ybtCIu)), col = rgb(0.8,0,0,0.2), border =NA)
abline(v=-0.5, lwd=3, lty=2)
pres <- predict(lrm0, type = "response")
segments(x0=x, y1 = pres, y0=obs, col="gray")

@
}

\only<4>{
<<logreg3, dev='tikz', echo=FALSE, fig.keep='last', fig.width=8, fig.height=6, out.width="0.5\\textwidth",out.height="0.5\\textheight">>=
setPar()
plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5))
abline(h=c(0,1))
abline(v=0)
ybt <- 1/(1+exp(-prlrm0$fit))
lines(x = prlrm0$x, ybt, lwd=5, col='red')

ybtCIu <- 1/(1+exp(-(prlrm0$fit+2*prlrm0$se.fit)))
ybtCIl <- 1/(1+exp(-(prlrm0$fit-2*prlrm0$se.fit)))

polygon(x=c(prlrm0$x,rev(prlrm0$x)), y=c(ybtCIl,rev(ybtCIu)), col = rgb(0.8,0,0,0.2), border =NA)
abline(v=-0.5, lwd=3, lty=2)
rect(xleft = c(-4, -0.5), ybottom = c(0,0.5), xright = c(-0.5,4), ytop = c(0.5,1), angle = 45, density = 10, col = "green")
rect(xleft = c(-4, -0.5), ybottom = c(0.5,0), xright = c(-0.5,4), ytop = c(1,0.5), angle = 135, density = 10, col = "red")
pres <- predict(lrm0, type = "response")
segments(x0=x, y1 = pres, y0=obs, col="gray")

@
}

\begin{block}{}
\begin{enumerate}[<+->]
  \item Response increase/decrease with increasing predictor?
  \item Estimate probability of 0/1 given a predictor value
  \item Predict 0/1 and classify predictor values ($\rightarrow$ Machine Learning)
\end{enumerate}
\end{block}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Exercise 1, fit a glm}

Load the dataset \texttt{survivalsize.csv}. It contains fake data of individual-based measurements of body size and of survival from the time of measurement to the next year. Look at a summary of the data and plot them. Do you think size affects survival?
Use the function glm() to fit a logistic regression. What should the \texttt{family} argument be? What is the direction of the effect of size on survival?

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Exercise 2, Model assumptions?}

In R some model assumptions of linear models are routinely checked using plot(lm()): residual normality, independance and homogeneous variance, and legerage. If you know about these diagnostics (and what the plots should ideally look like) check them for your glm. Should you worry?
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Model assumptions}
\begin{block}{Logistic regression assumes:}
\begin{itemize}[<+->]
  \item \textbf{Binary data}
  \item No unaccounted source of correlations in the date (e.g., pseudo-replication, spatial autocorrelations, phylogenetic signal\dots)
  \item (no error in the predictors)
  \item (no complete separation = only 0s or only 1s for some predictor level)
\end{itemize}
\end{block}

\pause

NO assumptions about the distribution of residuals (Normality, homoscedasticity).\\
BUT more assumptions in non-binary GLMs (proportions and count data)!!

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Back-transformation}
<<echo=FALSE>>=
survdat <- read.csv("survivalsize.csv") 
@

<<size='footnotesize'>>=
summary(glm(survival ~ 1 + size, data = survdat, family = "binomial"))
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Back-transformation}
\vspace{-0.5cm}
\begin{center}
\begin{tikzpicture}
\node (sc) at (-6, 0.5) {\textbf{Scales:}};
\uncover<2->{
  \node (mininf) at (-4,0) {$- \infty $};
  \node (maxinf) at (4,0) {$+ \infty $};
  \node (binf) at (-3,0) {};
  \node (tinf) at (3,0) {};
  \node (zer) at (0,0) {$0$};
  \draw (binf) -- (zer) -- (tinf);
  \draw[dashed] (binf)--(mininf);
  \draw[dashed] (tinf)--(maxinf);
  \node (modest) at (-6,0) {Model estimates};
}
\uncover<3->{
  \node (prb) at (-6,-2) {Probabilities};
  \node (minpr) at (-4,-2) {0};
  \node (maxpr) at (4,-2) {1};
  \node (zerp) at (0,-2) {$0.5$};
  \draw (minpr)-- (zerp) -- (maxpr);
}
\uncover<4->{
  \node (dat) at (-6,-4) {Data};
  \node (mind) at (-4,-4) {\large \textbf{0}};
  \node (maxd) at (4,-4) {\large \textbf{1}};
  \draw[dashed] (mind)--(maxd);
}

\uncover<5->{

\draw[->, draw=red] (zer) -- (zerp);
\draw[->, draw=red] (mininf) -- (minpr);
\draw[->, draw=red] (maxinf) -- (maxpr);

\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (-0.5,0) -- (-0.7,-2);
\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (0.5,0) -- (0.7,-2);

\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (-1,0) -- (-2,-2);
\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (1,0) -- (2,-2);


\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (-1.5,0) -- (-3.8,-2);
\draw[->, draw=red, shorten <= 2pt, shorten >= 2pt] (1.5,0) -- (3.8,-2);

}

\uncover<6->{
\draw[->, draw=red, dashed, line width=2pt] (zerp) -- (mind);
\draw[->, draw=red, dashed, line width=2pt] (zerp) -- (maxd);

\draw[->, draw=red, dashed, line width=4pt] (minpr) -- (mind);
\draw[->, draw=red, dashed, line width=4pt] (maxpr) -- (maxd);

\draw[->, draw=red, dashed, line width=3pt] (-3,-2) -- (mind);
\draw[->, draw=red, dashed, line width=0.5pt] (-3,-2) -- (maxd);

\draw[->, draw=red, dashed, line width=3pt] (3,-2) -- (maxd);
\draw[->, draw=red, dashed, line width=0.5pt] (3,-2) -- (mind);
}
\end{tikzpicture}
\end{center}

\uncover<7->{
\begin{block}{Conversion:}
\begin{itemize}
  \item from model to probability: $p=\frac{1}{1+\exp(-x)}$ or \texttt{plogis(x)}
  \item probability and data on same scale, but continuous/discrete
  \item $\exp(slope)$ = odd-ratio
\end{itemize}
\end{block}
}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Exercise 3, interpretation}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Exercise 4, visualize model}


\end{frame}
%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Take pictures!}
BDSI would like some pictures to illustrate their website. Is everybody okay with pictures?
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Clustering}

\includegraphics[width=0.8\textwidth]{figure/logreg3-1}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Clustering: what threshold?}
\centering
\only<1>{\includegraphics[width=0.5\textwidth]{figure/logreg3-1}}

\only<2>{
<<logreg4, dev='tikz', echo=FALSE, fig.keep='last', fig.width=8, fig.height=6, out.width="0.5\\textwidth", out.height="0.5\\textheight", fig.align='center'>>=
setPar()
brpt <- -1
trbrpt <- 1/(1+exp(-coef(lrm0)[1]-brpt*coef(lrm0)[2]))

plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5), main=paste0("p(break)=", round(trbrpt,3)))
abline(h=c(0,1))
abline(v=0)
ybt <- 1/(1+exp(-prlrm0$fit))
lines(x = prlrm0$x, ybt, lwd=5, col='red')

ybtCIu <- 1/(1+exp(-(prlrm0$fit+2*prlrm0$se.fit)))
ybtCIl <- 1/(1+exp(-(prlrm0$fit-2*prlrm0$se.fit)))

polygon(x=c(prlrm0$x,rev(prlrm0$x)), y=c(ybtCIl,rev(ybtCIu)), col = rgb(0.8,0,0,0.2), border =NA)
abline(v=brpt, lwd=3, lty=2)
rect(xleft = c(-4, brpt), ybottom = c(0,trbrpt), xright = c(brpt,4), ytop = c(trbrpt,1), angle = 45, density = 10, col = "green")
rect(xleft = c(-4, brpt), ybottom = c(trbrpt,0), xright = c(brpt,4), ytop = c(1,trbrpt), angle = 135, density = 10, col = "red")
pres <- predict(lrm0, type = "response")
segments(x0=x, y1 = pres, y0=obs, col="gray")

@
}

\only<3>{
<<logreg5, dev='tikz', echo=FALSE, fig.keep='last', fig.width=8, fig.height=6, out.width="0.5\\textwidth", out.height="0.5\\textheight", fig.align='center'>>=
setPar()
brpt <- 1
trbrpt <- 1/(1+exp(-coef(lrm0)[1]-brpt*coef(lrm0)[2]))

plot(x, obs, xlab="\\color{orange}{Predictor}", ylab="\\color{purple}{Response}", xlim = c(-3,3), ylim=c(-0.5,1.5), main=paste0("p(break)=", round(trbrpt,3)))
abline(h=c(0,1))
abline(v=0)
ybt <- 1/(1+exp(-prlrm0$fit))
lines(x = prlrm0$x, ybt, lwd=5, col='red')

ybtCIu <- 1/(1+exp(-(prlrm0$fit+2*prlrm0$se.fit)))
ybtCIl <- 1/(1+exp(-(prlrm0$fit-2*prlrm0$se.fit)))

polygon(x=c(prlrm0$x,rev(prlrm0$x)), y=c(ybtCIl,rev(ybtCIu)), col = rgb(0.8,0,0,0.2), border =NA)
abline(v=brpt, lwd=3, lty=2)
rect(xleft = c(-4, brpt), ybottom = c(0,trbrpt), xright = c(brpt,4), ytop = c(trbrpt,1), angle = 45, density = 10, col = "green")
rect(xleft = c(-4, brpt), ybottom = c(trbrpt,0), xright = c(brpt,4), ytop = c(1,trbrpt), angle = 135, density = 10, col = "red")
pres <- predict(lrm0, type = "response")
segments(x0=x, y1 = pres, y0=obs, col="gray")

@
}
\begin{block}{}
\begin{itemize}[<+->]
  \item Break at 50\% probability?
  \item Never miss a case? (but false positives!) e.g., Epidemic prevention
  \item Never false positive? (but miss some positives!) e.g., Presumption of innocence
\end{itemize}
\end{block}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%
 
 \begin{frame}{Exercise 5 }
 
 \pause
 
 \begin{exampleblock}{How good is the classification? What threshold to use?}
   \begin{itemize}
     \item Receiver Operating Characteristic and Area Under the Curve (ROC / AUC)
     \item E.g., \texttt{library(pROC)}
   \end{itemize}
   \includegraphics[width=0.3\textwidth]{figure/rocplot-1}
 \end{exampleblock}
%
 \end{frame}
%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}[fragile]{Mixed effect logistic regression}
GLM + random effect = GLMM\\
\pause
<<eval=FALSE>>=
library(lme4)
glmer(response ~ 1 + predictor + (1|group), family="binomial",
          data=dat)
@

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}{Mixed effect logistic regression}
\begin{columns}
\begin{column}{0.3\textwidth}
\includegraphics[height=0.8\textheight]{figure/backtransformation.pdf}
\end{column}
\begin{column}{0.7\textwidth}
\begin{itemize}
  \item There is no meaningful residual variance
  \item Random effect variance distorted by logit
  \item Lots of variance from random process on top of expected values
\end{itemize}
\end{column}
\end{columns}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Repeatability in GLMM}

In LMM: repeatability = variance random effect / (variance random effect + residual variance)\\

\pause

\begin{alertblock}{but in GLMM, especially logistic:}
\begin{itemize}
  \item On what scale to take the random effect?
  \item No residual variance! Is repeatability always 1??
\end{itemize}
\end{alertblock}

\vfill

\textbf{Exercise 6}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%
\end{document}