\documentclass[12pt,a4paper]{scrartcl}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tikz}
%\usepackage{silence}
\usepackage{mdframed}
%\WarningFilter{mdframed}{You got a bad break}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{listings}
\usepackage{color}
\colorlet{exampcol}{blue!10}
\usepackage{multicol}
\usepackage{booktabs}

\usepackage[noanswer]{exercise}%[noanswer]

\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=black
}

\title{Exercises for linear mixed effect models, part 3}
\date{\today}
\author{Timoth\'ee Bonnet}


\begin{document}

<<echo=FALSE, results='hide', message=FALSE>>=
options(digits = 4) 
library(lme4)
@

\maketitle

\tableofcontents
\ListOfExerciseInToc
\ExerciseLevelInToc{subsubsection}

\clearpage



\section{Random interactions}

\subsection{Random slopes}

<<eval=FALSE, echo=FALSE>>=

set.seed(123)
id <- 1:50
idint <- rnorm(length(id),0,0.5)
idslo <- rnorm(length(id),0,0.08)
idslo[50] <- 0.4
obs <- 1:10

dat <- rbind(expand.grid(obs=obs, id=id),data.frame(obs=NA, id=rep(50,times=500)))

dat$x <- rnorm(1000, mean = 3,sd = 1)
dat$y <- 1.9 + idint[dat$id] + dat$x*(-0.2 + idslo[dat$id]) + rnorm(nrow(dat), 0, 0.2)

dat <- dat[,c("x", "y", "id")]
names(dat) <- c("darkness", "detectability", "location")

summary(lm(detectability ~ 1 + darkness, data = dat) )
summary(lmm0 <-lmer(detectability ~ 1 + darkness + (1|location), data = dat) )
summary(lmer(detectability ~ 1 + darkness + (1+darkness|location), data = dat) )
plot(dat$darkness, dat$detectability)

write.csv(x = dat, file = "hares.csv", quote = FALSE, row.names = FALSE)
@

\begin{Exercise}[difficulty=1, title={Random slopes and unbalanced data}]
Load the dataset \texttt{hares.csv}. It contains (fake) measurements of snowshoe hare color (darkness) and their detectability against the background where they live. Measurements were taken in 50 different locations. We want to know whether darkness has an effect on detectability. 
\begin{enumerate}
  \item Fit a simple linear model of detectability on darkness. What is the effect?
  \item Add a random intercept to the previous model. Does it change the result quantitatively?
  \item Add a random slope. What do you see now?
\end{enumerate}
\end{Exercise}
\begin{Answer}
<<eval=FALSE>>=
dat <- read.csv("hares.csv")
summary(lm(detectability ~ 1 + darkness, data = dat) )
summary(lmer(detectability ~ 1 + darkness + (1|location), data = dat) )
summary(lmer(detectability ~ 1 + darkness + (1+darkness|location), data = dat) )
@
The direction of the slope changes when you add a random slope (but not when you add a random intercept).
\end{Answer}

\begin{Exercise}[difficulty=2, title={Visualize random slopes}]
Visualize the effect from the random slope model, that is, plot the relationship detectability over darkness for every location. Add the overall relationship (for instance extracted from a simple linear regression). You can use the functions \texttt{ranef()} and \texttt{fixef()}. Why did the fixed effect of darkness changed so much when you added the random slope? 
\end{Exercise}
\begin{Answer}
<<>>=
mhar <- lmer(detectability ~ 1 + darkness + (1+darkness|location), data = dat)

rhar <- ranef(mhar)$location
fhar <- fixef(mhar)

dark<- seq(from=min(dat$darkness), to=max(dat$darkness), length.out = 100)
#first we plot the MEDIAN relationship, that is, the slope to the 
# hypothetical location with random effects of zero

plot(x=dark, y=fhar[1]+dark*fhar[2], ylim=c(0,4), type="l", 
     lwd=5, lty=2, main="Mean effect (dotted),
     Median effect (dashed) and location slopes")

#then we plot slopes for each locations
cls <- rainbow(n = nrow(rhar))
for(i in 1:nrow(rhar))
{
  lines(x=dark, y=(fhar[1]+rhar[i,1])+#intercept for location i
          dark*(fhar[2]+rhar[i,2]),# slope for location i
    col=cls[i])
}

m0har <- lm(detectability ~ 1 + darkness, data = dat) 
abline(m0har, lwd=5, lty=3)

legend(x = "topleft", legend = c("Simple regression", "Random slope"),
       lwd=5, lty = c(3,2))
@

The problem with the simple regression comes from the imbalance in the data! One location has a positive relationship, while the 49 others have a negative relationship. The simple regression is also positive. Why does one location have so much weight?
Let's look at sample sizes per location:
<<>>=
table(dat$location)
@

510 observations come from location 50! That is why it weights so much in the simple regression. In contrast the random slope can tell the mean effect from the mediam slope.
\end{Answer}



\begin{Exercise}[difficulty=2, title={Does natural selection vary?}]
Load the dataset \texttt{AllM.txt}. It contains true data from the long term monitoring of a wild animal population. We are interested in quantifying natural selection on Weight. To simplify let's assume natural selection is the slope of \texttt{fitnessR} on \texttt{Weight}. 
\begin{enumerate}
  \item Fit a linear regression of fitnessR on Weight. Include \texttt{Age} as a predictor Is there evidence for selection?
  \item Change your model to a mixed model with year as a random intercept. Do you think fitnessR varies a lot among years?
  \item Now add a random slope on weight. How much variation is there in selection?
  \item Make a graph to visualize selection on different years (the function \texttt{ranef()} extract random effects) (you can make the graph for adults, for juveniles, or both).
  \item Looking at the estimated variance for the intercept and for Weight, which one looks more important? Is that your impression graphically? Why?
  \item Bonus: test for the statistical significance of the variation in selection (you can use \texttt{anova()} to compare two models).
\end{enumerate}
\end{Exercise}
\begin{Answer}
Careful here! The dataset is a text file, not a csv file! You need to load the data using \texttt(read.table()) with the argument \texttt{header=TRUE}!
<<>>=
library(lme4)
allm <- read.table("AllM.txt", header = TRUE)
@

1. Simple linear regression
<<>>=
mlr <- lm(fitnessR ~ 1 + Age + Weight, data = allm)
summary(mlr)
@
It looks like there is selection for heavier individuals.\\

2. Random intercept
<<>>=
mri <- lmer(fitnessR ~ 1+ Age  + Weight + (1|Year), data = allm)
summary(mri)
@
Some years have much higher or much lower fitness on average.\\

3.Random slope
<<>>=
mrs <- lmer(fitnessR ~ 1+ Age  + Weight  + (1+ Weight|Year), data = allm)
summary(mrs)
@
Selection may fluctuate (the variance component for the slope of Weight is not null), but it looks small and it is very correlated to the random intercept. So it is difficult to tell.\\

4.Visualize
<<>>=
wgt <- seq(10,50, by=0.1)
fixef(mrs)
plot(x=wgt, y=fixef(mrs)[1] + fixef(mrs)[3]* wgt, ylim = c(0,5),
     xlab="Weight", ylab="Fitness")
rde <- ranef(mrs)$Year
for (i in 1:nrow(rde))
{
  lines(x=wgt, y=fixef(mrs)[1]+  rde[i,1] + (fixef(mrs)[3]+rde[i,2])* wgt)
}
@

5. Statistical test
<<>>=
anova(mri,mrs)
@
You can compare the random intercept only model to the random intercept and random slope model to see if adding the latter improves the fit. The p-value is approximate only, but that doesn't matter because it is tiny anyways. 

\end{Answer}

\subsection{Random factor interaction}

<<randfactor, dev="tikz", fig.keep='last', echo=FALSE, results='hide', eval=FALSE>>=
set.seed(12356)
dat <- expand.grid(id=1:50, x=0:1, obs=1:5)
idi <- rnorm(50,3,0.4)
idb <- rnorm(50,-0.2,0.2)+0.2*idi
dat$y <- idi[dat$id] + dat$x*idb[dat$id] + rnorm(nrow(dat),0,0.01)
plot(dat$x,dat$y, xlim=c(-0.5,1.5), xaxt="n" , ylab="Response", xlab="")
axis(side = 1, at = 0:1, labels = c("Control", "Treatment"))
dat$treat <- c("control", "treatment")[dat$x+1]
write.csv(dat, file = "interfactor.csv", row.names = FALSE, quote = FALSE)
@

<<reacnorm1, dev="tikz", fig.keep='last', fig.width=6, fig.height=5, echo=FALSE, results='hide', eval=FALSE>>=
plot(dat$x,dat$y, xlim=c(-0.5,1.5), xaxt="n" , ylab="Response", xlab="",
     main="reaction norm view: variances")
axis(side = 1, at = 0:1, labels = c("Control", "Treatment"))
abline(v=0)
for(i in 1:50)
{
  segments(x0 = -0.05, x1=0, y0 = idi[i], col=rgb(0.7,0,1,0.7))
  lines(x=0:1, y= c(idi[i], idi[i]+idb[i]), col=rgb(0,0,1,0.7))
}
text(x = c(-0.3, 0.5), y = c(2.5, 2.5), 
     labels = c("\\rotatebox{90}{\\Large \\textbf{\\color{purple} Intercepts}}", 
                "\\rotatebox{90}{\\Large \\textbf{\\color{blue} Slopes}}"))
@

<<reacnorm2, dev="tikz", fig.keep='last', fig.width=4, fig.height=5, echo=FALSE, results='hide', eval=FALSE>>=
plot(idi, idb, xlab="\\Large \\color{purple} Intercept RE", 
     ylab="\\Large \\color{blue} Slope RE", main="Reaction norm view: correlation")

@

<<charst1, dev="tikz", fig.keep='last', fig.width=6, fig.height=5, echo=FALSE, results='hide', eval=FALSE>>=
plot(dat$x,dat$y, xlim=c(-0.5,1.5), xaxt="n" , ylab="Response", xlab="", 
     col=c("red", "orange")[dat$x+1], main="character state view: variances")
axis(side = 1, at = 0:1, labels = c("Control", "Treatment"))
for(i in 1:50)
{
  lines(x=0:1, y= c(idi[i], idi[i]+idb[i]))
}
text(x = c(-0.1, 1.1), y = c(2, 2), 
     labels = c("\\rotatebox{90}{\\Large \\textbf{\\color{red} Variance control}}", 
                "\\rotatebox{90}{\\Large \\textbf{\\color{orange} Variance treatment}}"))
@

<<charst2, dev="tikz", fig.keep='last', fig.width=4, fig.height=5, echo=FALSE, results='hide', eval=FALSE>>=
plot(3+idi, 3+idi+idb, xlab="\\Large \\color{red} Control RE", 
     ylab="\\Large \\color{orange} Treatment RE", main="Character state view: correlation")
@


\begin{Exercise}[difficulty=1, title={Random interaction with a factor}]
Load the file \texttt{interfactor.csv} and fit two random interaction modelw of y as a function of treat, using both the reaction norm and the character state approach. How do the estimate differ?
Use the functions \texttt{AIC()}, \texttt{fitted()} and \texttt{resid()} to compare the fit of the two models? What can you conclude?
\end{Exercise}
\begin{Answer}
<<>>=
dat <- read.csv("interfactor.csv")
library(lme4)
summary(mf1 <- lmer(y ~ 1 + treat + (1+treat|id), data = dat))
@

<<>>=
summary(mf2 <- lmer(y ~ 1 + treat + (0+treat|id), data = dat))

plot(fitted(mf1), fitted(mf2))
plot(resid(mf1), resid(mf2))
AIC(mf1)
AIC(mf2)
@
The two models are equivalent. Actually you can convert one into the other:

<<>>=
vc1 <- VarCorr(mf1)
vm1 <- matrix(vc1$id[1:4], nrow = 2)

vc2 <- VarCorr(mf2)
vm2 <- matrix(vc2$id[1:4], nrow = 2)

trs <- matrix(c(1,1, 0,1), nrow = 2)

trs %*% vm1 %*% t(trs)
vm2
@
\end{Answer}


<<echo=FALSE, eval=FALSE>>=
modb <- read.table("modbeetles.txt", header = T)
head(modb)


library(lme4)

summary(lmer(Wo ~ 1 + host + (1|IDf), data = modb))

summary(lmer(Wo ~ 1 + host + (1+host|IDf), data = modb))

anob <- modb[,c("IDf", "host", "Wo", "date.mated")]
names(anob) <- c("parent", "environment", "mass", "cage")

anob$parent <- as.numeric(anob$parent)
anob$cage <- as.numeric(anob$cage)
anob$mass <- (anob$mass - 1) *10 +rnorm(nrow(anob), 0, 0.1)
head(anob)

summary(lmer(mass ~ 1 + environment + (1+environment|parent) + (1|cage), data = anob))
summary(lmer(mass ~ 1 + environment + (0+environment|parent) + (1|cage), data = anob))

summary(lmm2 <- lmer(mass ~ 1 + environment + (0+environment|parent) + (1|cage), data = anob))

plot(ranef(lmm2)$parent)

trs <- matrix(c(1,1, 0,1), nrow = 2)
covie <- -0.58*sqrt(1.0830*6.4103)
vcovie <- matrix(c(6.4103, covie, covie, 1.0830), nrow = 2)

newvcov <- trs %*% vcovie %*% t(trs)
newcor <- newvcov[1,2]/sqrt(newvcov[1,1]*newvcov[2,2])

write.csv(anob, "Data/beetles.csv", row.names = FALSE, quote = FALSE)
@



\begin{Exercise}[difficulty=2, title={Beetles: build a model}]
Load the dataset "beetles.csv". It contains (fake) data from an (real) experiment on gene-by-environment interactions. The variable of interest is the mass of beetles born in two different environments, from different parents, and in different cages. Assuming that we can measure genetic varition with parent random effects, we wonder if different genomes respond differently to different environments.
\textbf{Build the model corresponding to this question in lme4.}

(hints: you could start from a lm() of mass modeled by environment, then add random intercepts, and finally a little something more).
\end{Exercise}

\begin{Exercise}[difficulty=2, title={Beetles: look at the model}]
What are the variances related to genetic differences? How are they correlated? Does genetic variation explain a lot of the total variation we observe? Try and draw a representation of genetic variation in the two environments. 
\end{Exercise}

\begin{Exercise}[difficulty=3, title={Beetles: interpret}]
Interpret model outputs (use raw numbers and / or graphes) to answer the following:
Is there evidence for genetic variation? Do the two environment differ in their effects on beetles? \\
Is there evidence for genetic variation in the response to the environment? \\
Does that mean that genomes good at environment 1 are bad at environment 2?
\end{Exercise}

\section{Correlated random effects}
In all of the above, we have assumed that random effect levels to be perfectly correlated (e.g., observations from the same year) or not at all correlated (e.g., observations from different years). It can be very interesting to allow for intermediate values, in particular for models of spatio-temporal autocorrelation, phylogenetics, quantitative genetics.


\subsection{Quantitative genetics}
Genes are transmitted from parents to offspring in a very predictable way: their genes have a correlation of 50\%. Therefore it is possible to estimate the genetic variance without any DNA sequencing when a dataset contains parents and offspring.
Going back to the long term monitoring of the wild animal population, let's load the population pedigree (ped.txt) and calculate this relatedness matrix:
<<Gmatrix, dev='tikz'>>=
library(MCMCglmm)
ped <- read.table("ped.txt", header = TRUE)
ainv <- inverseA(ped)$Ainv #the inverse of relatedness matrix
image(solve(ainv)[500:600,500:600], useRaster=T) #the relatedness matrix
@

Is there genetic variation in weight? Here is a demonstration of fitting a quantitative genetic model.
<<message=FALSE, cache=TRUE, eval=FALSE>>=
allm <- read.table("AllM.txt", header = TRUE)

mweight <- MCMCglmm(Weight ~ 1 + Age*Sex, random=~Year + id, 
                    ginverse = list(id=ainv), data=allm)
summary(mweight)
@
The effect "id" has a large variance attached to it, suggesting the presence of a lot of genetic variation.

\begin{Exercise}[difficulty=1, title={Is it really genetic?}]
However that was a bit cheating, because individuals had several observations, so the "genetic" random effect may be just repeated measurements. Let's add another random effect for individual, but not connected to the relatedness matrix (you can use the variable "animal" which is a duplicate of "id"). 

\end{Exercise}
\begin{Answer}
<<message=FALSE, eval=FALSE>>=
mweightG <- MCMCglmm(Weight ~ 1 + Age*Sex, random=~Year + id + animal,
                     ginverse = list(animal=ainv), data=allm, nitt = 30000)
summary(mweightG)
@

\end{Answer}


\subsection{Phylogenetic model}

In a phylogenetic tree some species have a longer common evolutionary history than others. What those species look like may be influenced by the common evolutionary history. We can model that by considering phylogenetic correlations between two lineage as the time of common evolution relative to their outgroup. 


\begin{Exercise}[difficulty=1, title={Demo of Phylogeny and correlated phenotypes}]
Load a phylogeny of bird families and some bird phenoypes:
<<>>=
load("birdfamilies") #phylogeny. Loading creates the object bird.families
birds <- read.csv("birdpheno.csv") #family phenotypes
@

To start, we subset the phylogeny to a few families
<<>>=
bird.families <- makeNodeLabel(bird.families)
some.families <- c("Certhiidae", "Paridae", "Gruidae",
"Struthionidae")
Nphylo <- drop.tip(bird.families, setdiff(bird.families$tip.label,
some.families))
@
So we can easily visualize it:
<<>>=
plot(Nphylo)
@
Which families will tend to have more similar phenotypes do you think?\\

We can calculate the variance covariance matrix of that tree as:
<<>>=
library(MCMCglmm)
INphylo <- inverseA(Nphylo)
sA <- as.matrix(solve(INphylo$Ainv))
colnames(sA) <- rownames(sA) <- rownames(INphylo$Ainv)
sA
@
What are the zero?\\

Coming back to the full dataset, we can fit a phylogenetic model as:
<<message=FALSE, eval=FALSE>>=
INphylofull <- inverseA(bird.families) # Object contains inverse relatedness matrix, pedigree, inbreeding...
prior0 <- list(G=list(G1=list(V=1, nu=1, alpha.mu=0, alpha.V=100)),
               R=list(V=1, nu=0.002))
m1 <- MCMCglmm(y ~ 1, random = ~id, ginverse = list(id=INphylofull$Ainv), 
               data = birds, prior = prior0)
summary(m1)
@
\end{Exercise}
\begin{Answer}

\end{Answer}

\end{document}
